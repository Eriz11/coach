{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Reinforcement Learning with Coach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many real-world problems do not have a simulator modelling the environment, that the agent would be interacting with, in standard Reinforcement Learning. Instead, all we have is some data that was collected using some deployed policy, and we would like to use that data in order to learn a better policy for solving the problem. \n",
    "One such example might be developing a better drug dose and/or admission schedule policy. We have data based on the policy that was used with patients so far, but cannot experiment (and explore) on patients to collect new data. \n",
    "\n",
    "But, wait... If we don't have a simulator, how would we evaluate our newly learned policy and know if it is any good? Which algorithms should we be using in order to better address the problem of learning from a batch of data? Alternatively, what do we do if we don't have a simulator, but instead can actually deploy our policy on that real-world problem, and would just like to separate the new data collection from the learning a new policy part (i.e. if we have a system that can quite easily run inference, but is very hard to integrate a reinforcement learning framework with, such as Coach, for learning a new policy. **Can we have an actual example here? i.e. something like an oil company with nodes that can only do inference**), and iteratively train a better and better policy? \n",
    "\n",
    "We will try to address these questions and more in this tutorial, demonstrating how to use batch reinforcement learning. \n",
    "\n",
    "First, let's use some simple environment to collect the data to be used for learning an agent using BatchRL. In reality, we probably would already have a dataset of transitions of the form `<current_observation, action, reward, next_state>` to be used for learning a new agent. Ideally, we would also have, for each transtion, p(a|o) the probabilty of taking that action, given that transition's `current_observation`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "First, get the required imports and other general settings going for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'BatchRL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dcf4f1686713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Get all the outputs of this tutorial out on the 'BatchRL' folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BatchRL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# the dataset size to collect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BatchRL'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from rl_coach.agents.dqn_agent import DQNAgentParameters\n",
    "from rl_coach.agents.ddqn_bcq_agent import DDQNBCQAgentParameters, KNNParameters\n",
    "from rl_coach.base_parameters import VisualizationParameters\n",
    "from rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, CsvDataset\n",
    "from rl_coach.environments.gym_environment import GymVectorEnvironment\n",
    "from rl_coach.graph_managers.batch_rl_graph_manager import BatchRLGraphManager\n",
    "from rl_coach.graph_managers.graph_manager import ScheduleParameters\n",
    "from rl_coach.memories.memory import MemoryGranularity\n",
    "from rl_coach.schedules import LinearSchedule\n",
    "from rl_coach.memories.episodic import EpisodicExperienceReplayParameters\n",
    "from rl_coach.architectures.head_parameters import QHeadParameters\n",
    "from rl_coach.agents.ddqn_agent import DDQNAgentParameters\n",
    "from rl_coach.base_parameters import TaskParameters\n",
    "from rl_coach.spaces import SpacesDefinition, DiscreteActionSpace, VectorObservationSpace, StateSpace, RewardSpace\n",
    "\n",
    "# Get all the outputs of this tutorial out on the 'BatchRL' folder\n",
    "os.chdir('BatchRL')\n",
    "\n",
    "# the dataset size to collect \n",
    "DATASET_SIZE = 50000\n",
    "\n",
    "task_parameters = TaskParameters(experiment_path='.')\n",
    "\n",
    "####################\n",
    "# Graph Scheduling #\n",
    "####################\n",
    "\n",
    "schedule_params = ScheduleParameters()\n",
    "\n",
    "# 100 epochs (we run train over all the dataset every epoch) of training\n",
    "schedule_params.improve_steps = TrainingSteps(100)\n",
    "\n",
    "# we evaluate the model every epoch\n",
    "schedule_params.steps_between_evaluation_periods = TrainingSteps(1)\n",
    "\n",
    "schedule_params.evaluation_steps = EnvironmentEpisodes(10)\n",
    "schedule_params.heatup_steps = EnvironmentSteps(DATASET_SIZE)\n",
    "\n",
    "################\n",
    "#  Environment #\n",
    "################\n",
    "env_params = GymVectorEnvironment(level='Acrobot-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Acrobot with Batch RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Gym's `Acrobot-v1` in order to collect a dataset of experience, and then use that dataset in order to learn an agent solving the environment using Batch RL. \n",
    "\n",
    "### The Preset \n",
    "\n",
    "First we will collect a dataset using a random action selecting policy. Then we will use that dataset to train an agent in a Batch RL fashion. <br>\n",
    "Let's start simple - training an agent with Double DQN. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;46mCreating graph - name: BatchRLGraphManager\u001b[0m\n",
      "\u001b[30;46mCreating agent - name: agent\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d9382d31e7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mvis_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVisualizationParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump_signals_to_csv_every_x_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     reward_model_num_epochs=30)\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mgraph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mgraph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimprove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m(self, task_parameters)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# create the graph modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# set self as the parent of all the level managers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/graph_managers/batch_rl_graph_manager.py\u001b[0m in \u001b[0;36m_create_graph\u001b[0;34m(self, task_parameters)\u001b[0m\n\u001b[1;32m    148\u001b[0m         level_manager = LevelManager(agents=agents,\n\u001b[1;32m    149\u001b[0m                                      \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"main_level\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                                      spaces_definition=self.spaces_definition)\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlevel_manager\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/level_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, agents, environment, real_environment, steps_limit, should_reset_agent_state_after_time_limit_passes, spaces_definition)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironmentSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The num consecutive steps for acting must be defined in terms of environment steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspaces_definition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# there are cases where we don't have an environment. e.g. in batch-rl or in imitation learning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/level_manager.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, spaces_definition)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces_definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_environment_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/level_manager.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces_definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_environment_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36mset_environment_parameters\u001b[0;34m(self, spaces)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_target_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_environment_dependent_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNetworkWrapper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/agents/value_optimization_agent.py\u001b[0m in \u001b[0;36minit_environment_dependent_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_environment_dependent_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_environment_dependent_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiscreteActionSpace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36minit_environment_dependent_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;31m# create all the networks of the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36mcreate_networks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                     \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                                                     \u001b[0mreplicated_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicated_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                                                     worker_device=self.worker_device)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_networks_summary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/architectures/network_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_parameters, has_target, has_global, name, spaces, replicated_device, worker_device)\u001b[0m\n\u001b[1;32m     84\u001b[0m                                               \u001b[0mnetwork_is_local\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                                               \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                                               network_is_trainable=True)\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Target network - a local, slow updating network used for stabilizing the learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/architectures/tensorflow_components/general_network.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(variable_scope, devices, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliary_name_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mconstruct_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliary_name_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/architectures/tensorflow_components/general_network.py\u001b[0m in \u001b[0;36mconstruct_on_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneralTensorFlowNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mGeneralTensorFlowNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# If variable_scope is in our dictionary, then this is not the first time that this variable_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/architectures/tensorflow_components/general_network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_parameters, spaces, name, global_network, network_is_local, network_is_trainable)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         super().__init__(agent_parameters, spaces, name, global_network,\n\u001b[0;32m--> 126\u001b[0;31m                          network_is_local, network_is_trainable)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_return_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_available_return_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/coach/rl_coach/architectures/tensorflow_components/architecture.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_parameters, spaces, name, global_network, network_is_local, network_is_trainable)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         with tf.variable_scope(\"/\".join(self.name.split(\"/\")[1:]), initializer=tf.contrib.layers.xavier_initializer(),\n\u001b[0m\u001b[1;32m    101\u001b[0m                                custom_getter=local_getter if network_is_local and global_network else None):\n\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/learn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/learn/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m'tf.contrib.learn.basic_session_run_hooks.NanTensorHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;34m'tf.train.NanTensorHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     basic_session_run_hooks.NanTensorHook)\n\u001b[0m\u001b[1;32m     54\u001b[0m SummarySaverHook = deprecated_alias(\n\u001b[1;32m     55\u001b[0m     \u001b[0;34m'tf.contrib.learn.basic_session_run_hooks.SummarySaverHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mdeprecated_alias\u001b[0;34m(deprecated_name, name, func_or_class, warn_once)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Make a new class with __init__ wrapped in a warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mclass\u001b[0m \u001b[0m_NewClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m       __doc__ = decorator_utils.add_notice_to_docstring(\n\u001b[1;32m    175\u001b[0m           \u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please use %s instead.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36m_NewClass\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m                            'It will be removed in a future version. '])\n\u001b[1;32m    179\u001b[0m       \u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0m__module__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;34m@\u001b[0m\u001b[0m_wrap_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36m_call_location\u001b[0;34m(outer)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;34m\"\"\"Returns call location given level up from current call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# CPython internals are available, use them for performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/coach_env/lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    742\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__loader__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;31m# or it is in the linecache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mismodule\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0m__doc__\u001b[0m         \u001b[0mdocumentation\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         __file__        filename (missing for built-in modules)\"\"\"\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # just to clean things up; only needed for the tutorial\n",
    "\n",
    "#########\n",
    "# Agent #\n",
    "#########\n",
    "agent_params = DDQNAgentParameters()\n",
    "agent_params.network_wrappers['main'].batch_size = 128\n",
    "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(100)\n",
    "agent_params.algorithm.discount = 0.99\n",
    "\n",
    "# to jump start the agent's q values, and speed things up, we'll initialize the last Dense layer\n",
    "# with something in the order of the discounted reward of a random policy\n",
    "agent_params.network_wrappers['main'].heads_parameters = \\\n",
    "[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n",
    "\n",
    "# NN configuration\n",
    "agent_params.network_wrappers['main'].learning_rate = 0.0001\n",
    "agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n",
    "\n",
    "# ER - we'll be needing an episodic replay buffer for off-policy evaluation\n",
    "agent_params.memory = EpisodicExperienceReplayParameters()\n",
    "\n",
    "# E-Greedy schedule - there is no exploration in Batch RL. Disabling E-Greedy. \n",
    "agent_params.exploration.epsilon_schedule = LinearSchedule(initial_value=0, final_value=0, decay_steps=1)\n",
    "agent_params.exploration.evaluation_epsilon = 0\n",
    "\n",
    "\n",
    "graph_manager = BatchRLGraphManager(agent_params=agent_params,\n",
    "                                    env_params=env_params,\n",
    "                                    schedule_params=schedule_params,\n",
    "                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n",
    "                                    reward_model_num_epochs=30)\n",
    "graph_manager.create_graph(task_parameters)\n",
    "graph_manager.improve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the prints around off-policy evaluation for now (we'll return to it in a bit), we first see Coach running a long heatup of 50,000 steps (as we have defined a `DATASET_SIZE` of 50,000 in the preliminaries section), in order to collect a dataset of random actions. Then, we can see Coach training a supervised reward model that is needed for OPE (off-policy evaluation). Last, Coach starts using the collected dataset of experience to train a Double DQN agent. Since, for this environment, we actually do have a simulator, Coach will be using it to evaluate the learned policy. As you can probably see, since this is a very simple environment, a dataset of just random actions is enough to get a Double DQN agent training, and reaching rewards of less than -100 (actually solving the environment). As you can also probably notice, the learning is not very stable, and if you'll take a look at the Q values predicted by the agent (e.g. in Coach Dashboard; this tutorial experiment results are under the `tutorials` folder), you will be seeing them unboundedly increasing. This is caused due to the Batch RL based learning where not interacting with the environment any further, while randomly exposing only small parts of the MDP in the dataset, makes learning even harder than standard Off-Policy RL. This phenomena is very nicely explained in [Off-Policy Deep Reinforcement Learning without Exploration](https://arxiv.org/abs/1812.02900). We have implemented a discrete-actions variant of [Batch Constrained Q-Learning](https://github.com/NervanaSystems/coach/blob/master/rl_coach/agents/ddqn_bcq_agent.py), which helps with mitigating this issue. \n",
    "\n",
    "Next, let's switch to a dataset containing data combined from several 'deployed' policies, as is often the case in real-world scenarios, where we usually already have some policy (hopefully not a random one) in-place and we want to improve upon. For instance, a recommender system already using some policy for gemerating recommendations, and we want to use Batch RL to acquire a better and improved policy. <br>\n",
    "\n",
    "We will demonstrate that by training an agent, and using its replay buffer content as the dataset from which we will learn a new agent, based solely on that data (without any further interaction with the environment). This should allow for both a better trained agent and for more meaningful Off-Policy Evaluation (as the more extensive your input data is, i.e. exposing more of the MDP, the better is the evaluation of a new agent based on it. In other words - garbage-in, garbage-out).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # just to clean things up; only needed for the tutorial\n",
    "\n",
    "# Experience Generating Agent parameters\n",
    "experience_generating_agent_params = DDQNAgentParameters()\n",
    "\n",
    "# schedule parameters\n",
    "experience_generating_schedule_params = ScheduleParameters()\n",
    "experience_generating_schedule_params.heatup_steps = EnvironmentSteps(1000)\n",
    "experience_generating_schedule_params.improve_steps = TrainingSteps(\n",
    "    DATASET_SIZE - experience_generating_schedule_params.heatup_steps.num_steps)\n",
    "experience_generating_schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\n",
    "experience_generating_schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n",
    "\n",
    "# DQN params\n",
    "experience_generating_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\n",
    "experience_generating_agent_params.algorithm.discount = 0.99\n",
    "experience_generating_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n",
    "\n",
    "# NN configuration\n",
    "experience_generating_agent_params.network_wrappers['main'].learning_rate = 0.0001\n",
    "experience_generating_agent_params.network_wrappers['main'].batch_size = 128\n",
    "experience_generating_agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n",
    "experience_generating_agent_params.network_wrappers['main'].heads_parameters = \\\n",
    "[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n",
    "\n",
    "# ER size\n",
    "experience_generating_agent_params.memory = EpisodicExperienceReplayParameters()\n",
    "experience_generating_agent_params.memory.max_size = \\\n",
    "    (MemoryGranularity.Transitions,\n",
    "     experience_generating_schedule_params.heatup_steps.num_steps +\n",
    "     experience_generating_schedule_params.improve_steps.num_steps)\n",
    "\n",
    "# E-Greedy schedule\n",
    "experience_generating_agent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, DATASET_SIZE)\n",
    "experience_generating_agent_params.exploration.evaluation_epsilon = 0\n",
    "\n",
    "# 50 epochs (we run train over all the dataset every epoch) of training\n",
    "schedule_params.improve_steps = TrainingSteps(50)\n",
    "\n",
    "graph_manager = BatchRLGraphManager(agent_params=agent_params,\n",
    "                                    experience_generating_agent_params=experience_generating_agent_params,\n",
    "                                    experience_generating_schedule_params=experience_generating_schedule_params,\n",
    "                                    env_params=env_params,\n",
    "                                    schedule_params=schedule_params,\n",
    "                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n",
    "                                    reward_model_num_epochs=30,\n",
    "                                    train_to_eval_ratio=0.5)\n",
    "graph_manager.create_graph(task_parameters)\n",
    "graph_manager.improve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy Evaluation\n",
    "As we have mentioned earlier, one of the hardest problems in Batch RL is that we do not have a simulator or cannot easily deploy a trained policy on the real-problem system, in order to test its goodness. This is where off-policy evaluation (OPE) comes in handy. </br>\n",
    "\n",
    "Coach supports several off-policy evaluators, some are aimed at bandits problems (thus the evaluators are only evaluating a single step return. Yet from our experience, there are cases where these also correlate with the return of RL problems), and others are for full-blown Reinforcement Learning problems. The main goal of the OPEs is to help us select the best model to deploy, either for collecting more data to do another round of Batch RL on, or to select our final model for deployment. \n",
    "\n",
    "Openning the experiment that we have just ran (under the `tutorials` folder, with Coach Dashboard), you will be able to plot the actual simulator's `Evaluation Reward`. Usually, we won't have this signal available since we won't have a simulator, but since we're using just a dummy environment, for demonstration purposes, we can use it to test for how some of the OPEs correlate with it. \n",
    "\n",
    "Here are two example plots from Dashboard showing how well the `Weighted Importance Sampling` (RL estimator) and the `Doubly Robust` (bandits estimator) each correlate with the `Evaluation Reward`. </br>\n",
    "![Weighted Importance Sampling](BatchRL/img/wis.png \"Weighted Importance Sampling vs. Evaluation Reward\") \n",
    "</br>\n",
    "![Doubly Robust](BatchRL/img/dr.png \"Doubly Robust vs. Evaluation Reward\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a dataset to feed a Batch RL algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now understand how things are expected to work. But, hey... if we don't have a simulator (which we did have in this tutorial so far, and have used it to generate a training/evaluation dataset) how will we feed Coach with the dataset to train/evaluate on?\n",
    "\n",
    "### The CSV\n",
    "Coach defines an expected csv data, format from which it will read and fill its replay buffer. We have created an example csv from the same `Acrobot-v1` environment, and have placed it under the `tutorials` folder.\n",
    "\n",
    "Here are the first couple of lines from it so you can get a grip of what to expect - \n",
    "\n",
    "| action | all_action_probabilities | episode_id | episode_name | reward | transition_number | state_feature_0 | state_feature_1 | state_feature_2 | state_feature_3 | state_feature_4 | state_feature_5 \n",
    "|---|---|---|---|---|---|---|---|---|---|---|---------------------------------------------------------------------------|\n",
    "|0|[0.4159157,0.23191088,0.35217342]|0|acrobot|-1|0|0.996893843|0.078757007|0.997566524|0.069721088|-0.078539907|-0.072449002 |\n",
    "|1|[0.46244532,0.22402011,0.31353462]|0|acrobot|-1|1|0.997643051|0.068617369|0.999777604|0.021088905|-0.022653483|-0.40743716|\n",
    "|0|[0.4961428,0.21575058,0.2881066]|0|acrobot|-1|2|0.997613067|0.069051922|0.996147629|-0.087692077|0.023128103|-0.662019594|\n",
    "|0|[0.49341106,0.22363988,0.28294897]|0|acrobot|-1|3|0.997141344|0.075558854|0.972780655|-0.231727853|0.035575821|-0.771402023|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;46mCreating graph - name: BatchRLGraphManager\u001b[0m\n",
      "\u001b[30;46mCreating agent - name: agent\u001b[0m\n",
      "\u001b[30;46mLoading a replay buffer from a CSV file. CSV file path: acrobot_dataset.csv\u001b[0m\n",
      "Progress: (30/30) Time: 4.43 sec 100%|##########|  \n",
      "\u001b[30;46mTraining a regression model for estimating MDP rewards\u001b[0m\n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m0 \u001b[94mReward Model Loss: \u001b[0m78.00557143860902 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m1 \u001b[94mReward Model Loss: \u001b[0m76.14748372554084 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m2 \u001b[94mReward Model Loss: \u001b[0m73.58515795910306 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m3 \u001b[94mReward Model Loss: \u001b[0m70.04382876116614 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m4 \u001b[94mReward Model Loss: \u001b[0m65.50568371718522 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m5 \u001b[94mReward Model Loss: \u001b[0m59.98599805447101 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m6 \u001b[94mReward Model Loss: \u001b[0m53.793562782002006 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m7 \u001b[94mReward Model Loss: \u001b[0m46.68347980979582 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m8 \u001b[94mReward Model Loss: \u001b[0m39.663072881851605 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m9 \u001b[94mReward Model Loss: \u001b[0m32.85732336419847 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m10 \u001b[94mReward Model Loss: \u001b[0m26.461852166857835 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m11 \u001b[94mReward Model Loss: \u001b[0m21.540698728807122 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m12 \u001b[94mReward Model Loss: \u001b[0m17.621876851528317 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m13 \u001b[94mReward Model Loss: \u001b[0m15.195616310872554 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m14 \u001b[94mReward Model Loss: \u001b[0m13.468999570587856 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m15 \u001b[94mReward Model Loss: \u001b[0m12.277606643283605 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m16 \u001b[94mReward Model Loss: \u001b[0m11.3581288086887 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m17 \u001b[94mReward Model Loss: \u001b[0m10.47455353243118 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m18 \u001b[94mReward Model Loss: \u001b[0m9.640064304409297 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m19 \u001b[94mReward Model Loss: \u001b[0m8.924687097648654 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m20 \u001b[94mReward Model Loss: \u001b[0m8.139392695151084 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m21 \u001b[94mReward Model Loss: \u001b[0m7.49708370937557 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m22 \u001b[94mReward Model Loss: \u001b[0m6.892411991874164 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m23 \u001b[94mReward Model Loss: \u001b[0m6.204052922791744 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m24 \u001b[94mReward Model Loss: \u001b[0m5.623411105336294 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m25 \u001b[94mReward Model Loss: \u001b[0m5.081937301257425 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m26 \u001b[94mReward Model Loss: \u001b[0m4.635404943782378 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m27 \u001b[94mReward Model Loss: \u001b[0m4.097076245402038 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m28 \u001b[94mReward Model Loss: \u001b[0m3.649708044546347 \n",
      "\u001b[95mTraining Batch RL Models\u001b[0m - \u001b[94mEpoch: \u001b[0m29 \u001b[94mReward Model Loss: \u001b[0m3.2774464890512958 \n",
      "\u001b[30;46mCollecting static statistics for OPE\u001b[0m\n",
      "\u001b[30;46mStarting to improve batch_rl_graph task index 0\u001b[0m\n",
      "INFO:tensorflow:./checkpoint/0_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/0_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m1 \u001b[94mIPS: \u001b[0m-0.8495365087149273 \u001b[94mDM: \u001b[0m-7.047114372253418 \u001b[94mDR: \u001b[0m-4.2798212982346 \u001b[94mWIS: \u001b[0m-99.3429516954312 \u001b[94mSequential-DR: \u001b[0m-99.78588495999767 \n",
      "INFO:tensorflow:./checkpoint/1_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/1_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m2 \u001b[94mIPS: \u001b[0m-0.8442398987557099 \u001b[94mDM: \u001b[0m-6.9531660079956055 \u001b[94mDR: \u001b[0m-3.9628735785024167 \u001b[94mWIS: \u001b[0m-99.34295167072192 \u001b[94mSequential-DR: \u001b[0m-99.81795328193382 \n",
      "INFO:tensorflow:./checkpoint/2_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/2_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m3 \u001b[94mIPS: \u001b[0m-0.8482874498308146 \u001b[94mDM: \u001b[0m-7.031990051269531 \u001b[94mDR: \u001b[0m-3.961180905541561 \u001b[94mWIS: \u001b[0m-99.34295167916319 \u001b[94mSequential-DR: \u001b[0m-99.83047204203324 \n",
      "INFO:tensorflow:./checkpoint/3_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/3_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m4 \u001b[94mIPS: \u001b[0m-0.8552728025650856 \u001b[94mDM: \u001b[0m-7.098641872406006 \u001b[94mDR: \u001b[0m-4.07582160974944 \u001b[94mWIS: \u001b[0m-99.34295168013074 \u001b[94mSequential-DR: \u001b[0m-99.80110351192205 \n",
      "INFO:tensorflow:./checkpoint/4_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/4_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m5 \u001b[94mIPS: \u001b[0m-0.8516336880049862 \u001b[94mDM: \u001b[0m-7.036120891571045 \u001b[94mDR: \u001b[0m-3.8683753350470313 \u001b[94mWIS: \u001b[0m-99.34295158527715 \u001b[94mSequential-DR: \u001b[0m-99.74708956295828 \n",
      "INFO:tensorflow:./checkpoint/5_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/5_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m6 \u001b[94mIPS: \u001b[0m-0.8485707710107394 \u001b[94mDM: \u001b[0m-7.010881423950195 \u001b[94mDR: \u001b[0m-3.7569277608368967 \u001b[94mWIS: \u001b[0m-99.34295139870692 \u001b[94mSequential-DR: \u001b[0m-99.70709771311043 \n",
      "INFO:tensorflow:./checkpoint/6_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/6_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m7 \u001b[94mIPS: \u001b[0m-0.8579104918652063 \u001b[94mDM: \u001b[0m-7.127741813659668 \u001b[94mDR: \u001b[0m-3.9425971441768364 \u001b[94mWIS: \u001b[0m-99.34295165811825 \u001b[94mSequential-DR: \u001b[0m-99.72426414359681 \n",
      "INFO:tensorflow:./checkpoint/7_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/7_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m8 \u001b[94mIPS: \u001b[0m-0.8528249620505385 \u001b[94mDM: \u001b[0m-7.073077201843262 \u001b[94mDR: \u001b[0m-3.7719375704433555 \u001b[94mWIS: \u001b[0m-99.34295150730854 \u001b[94mSequential-DR: \u001b[0m-99.60488858484479 \n",
      "INFO:tensorflow:./checkpoint/8_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/8_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m9 \u001b[94mIPS: \u001b[0m-0.8504070848727981 \u001b[94mDM: \u001b[0m-7.0357537269592285 \u001b[94mDR: \u001b[0m-3.725525626178567 \u001b[94mWIS: \u001b[0m-99.34295079986893 \u001b[94mSequential-DR: \u001b[0m-99.58503565765588 \n",
      "INFO:tensorflow:./checkpoint/9_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/9_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m10 \u001b[94mIPS: \u001b[0m-0.8596742549626468 \u001b[94mDM: \u001b[0m-7.138176441192627 \u001b[94mDR: \u001b[0m-3.7836433724752725 \u001b[94mWIS: \u001b[0m-99.3429511210915 \u001b[94mSequential-DR: \u001b[0m-99.64466563747175 \n",
      "INFO:tensorflow:./checkpoint/10_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/10_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m11 \u001b[94mIPS: \u001b[0m-0.8526645807477523 \u001b[94mDM: \u001b[0m-7.021304607391357 \u001b[94mDR: \u001b[0m-3.6237278766048266 \u001b[94mWIS: \u001b[0m-99.34294308974296 \u001b[94mSequential-DR: \u001b[0m-99.49696136052017 \n",
      "INFO:tensorflow:./checkpoint/11_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/11_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m12 \u001b[94mIPS: \u001b[0m-0.8522283661964188 \u001b[94mDM: \u001b[0m-7.0461626052856445 \u001b[94mDR: \u001b[0m-3.648273770359455 \u001b[94mWIS: \u001b[0m-99.34294697460152 \u001b[94mSequential-DR: \u001b[0m-99.4656989029425 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:./checkpoint/12_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/12_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m13 \u001b[94mIPS: \u001b[0m-0.8652212601989943 \u001b[94mDM: \u001b[0m-7.195096969604492 \u001b[94mDR: \u001b[0m-3.770010014715164 \u001b[94mWIS: \u001b[0m-99.34293820733853 \u001b[94mSequential-DR: \u001b[0m-99.68674557258258 \n",
      "INFO:tensorflow:./checkpoint/13_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/13_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m14 \u001b[94mIPS: \u001b[0m-0.8552962526076159 \u001b[94mDM: \u001b[0m-7.0609235763549805 \u001b[94mDR: \u001b[0m-3.637347618944837 \u001b[94mWIS: \u001b[0m-99.34292020325171 \u001b[94mSequential-DR: \u001b[0m-99.36788101930414 \n",
      "INFO:tensorflow:./checkpoint/14_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/14_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m15 \u001b[94mIPS: \u001b[0m-0.8550920400128919 \u001b[94mDM: \u001b[0m-7.037744522094727 \u001b[94mDR: \u001b[0m-3.5705762026017904 \u001b[94mWIS: \u001b[0m-99.34289964763634 \u001b[94mSequential-DR: \u001b[0m-99.39322828693764 \n",
      "INFO:tensorflow:./checkpoint/15_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/15_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m16 \u001b[94mIPS: \u001b[0m-0.8637585805906306 \u001b[94mDM: \u001b[0m-7.1581597328186035 \u001b[94mDR: \u001b[0m-3.627553118276358 \u001b[94mWIS: \u001b[0m-99.34232596737372 \u001b[94mSequential-DR: \u001b[0m-99.48242332808918 \n",
      "INFO:tensorflow:./checkpoint/16_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/16_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m17 \u001b[94mIPS: \u001b[0m-0.8610628397124483 \u001b[94mDM: \u001b[0m-7.121901035308838 \u001b[94mDR: \u001b[0m-3.6100988548611372 \u001b[94mWIS: \u001b[0m-99.34253367208822 \u001b[94mSequential-DR: \u001b[0m-99.35525637581901 \n",
      "INFO:tensorflow:./checkpoint/17_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/17_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m18 \u001b[94mIPS: \u001b[0m-0.858839808001823 \u001b[94mDM: \u001b[0m-7.082253456115723 \u001b[94mDR: \u001b[0m-3.5532022345589565 \u001b[94mWIS: \u001b[0m-99.34078107303763 \u001b[94mSequential-DR: \u001b[0m-99.31875119330832 \n",
      "INFO:tensorflow:./checkpoint/18_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/18_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m19 \u001b[94mIPS: \u001b[0m-0.8660876623107885 \u001b[94mDM: \u001b[0m-7.177270889282227 \u001b[94mDR: \u001b[0m-3.5942214772578756 \u001b[94mWIS: \u001b[0m-99.3367689428125 \u001b[94mSequential-DR: \u001b[0m-99.3976382269785 \n",
      "INFO:tensorflow:./checkpoint/19_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/19_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m20 \u001b[94mIPS: \u001b[0m-0.8590682443394233 \u001b[94mDM: \u001b[0m-7.0575151443481445 \u001b[94mDR: \u001b[0m-3.4719437619512687 \u001b[94mWIS: \u001b[0m-99.33953383958563 \u001b[94mSequential-DR: \u001b[0m-99.2041480600046 \n",
      "INFO:tensorflow:./checkpoint/20_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/20_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m21 \u001b[94mIPS: \u001b[0m-0.858433319343843 \u001b[94mDM: \u001b[0m-7.068203449249268 \u001b[94mDR: \u001b[0m-3.554060369310248 \u001b[94mWIS: \u001b[0m-99.3392285087406 \u001b[94mSequential-DR: \u001b[0m-99.18298352727237 \n",
      "INFO:tensorflow:./checkpoint/21_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/21_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m22 \u001b[94mIPS: \u001b[0m-0.8670448093239687 \u001b[94mDM: \u001b[0m-7.182616710662842 \u001b[94mDR: \u001b[0m-3.444602472053612 \u001b[94mWIS: \u001b[0m-97.94270607556875 \u001b[94mSequential-DR: \u001b[0m-99.33312146776534 \n",
      "INFO:tensorflow:./checkpoint/22_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/22_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m23 \u001b[94mIPS: \u001b[0m-0.8622972917456433 \u001b[94mDM: \u001b[0m-7.1389641761779785 \u001b[94mDR: \u001b[0m-3.430806707080556 \u001b[94mWIS: \u001b[0m-99.2792074604079 \u001b[94mSequential-DR: \u001b[0m-99.1855429769436 \n",
      "INFO:tensorflow:./checkpoint/23_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/23_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m24 \u001b[94mIPS: \u001b[0m-0.8614535586530994 \u001b[94mDM: \u001b[0m-7.0965681076049805 \u001b[94mDR: \u001b[0m-3.450586213271706 \u001b[94mWIS: \u001b[0m-99.27671856819082 \u001b[94mSequential-DR: \u001b[0m-99.13414715767192 \n",
      "INFO:tensorflow:./checkpoint/24_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/24_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m25 \u001b[94mIPS: \u001b[0m-0.8661722363001229 \u001b[94mDM: \u001b[0m-7.1503705978393555 \u001b[94mDR: \u001b[0m-3.4271264433828983 \u001b[94mWIS: \u001b[0m-98.07631614231707 \u001b[94mSequential-DR: \u001b[0m-99.1500823691124 \n",
      "INFO:tensorflow:./checkpoint/25_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/25_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m26 \u001b[94mIPS: \u001b[0m-0.862025998940245 \u001b[94mDM: \u001b[0m-7.064321517944336 \u001b[94mDR: \u001b[0m-3.385709338039685 \u001b[94mWIS: \u001b[0m-99.2799063327563 \u001b[94mSequential-DR: \u001b[0m-99.03259274073889 \n",
      "INFO:tensorflow:./checkpoint/26_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/26_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m27 \u001b[94mIPS: \u001b[0m-0.8642038553096838 \u001b[94mDM: \u001b[0m-7.16956090927124 \u001b[94mDR: \u001b[0m-3.4952877867683 \u001b[94mWIS: \u001b[0m-98.9138654085852 \u001b[94mSequential-DR: \u001b[0m-99.08959123705935 \n",
      "INFO:tensorflow:./checkpoint/27_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/27_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m28 \u001b[94mIPS: \u001b[0m-0.8654076455438385 \u001b[94mDM: \u001b[0m-7.124244213104248 \u001b[94mDR: \u001b[0m-3.4130926304613145 \u001b[94mWIS: \u001b[0m-98.04693278741014 \u001b[94mSequential-DR: \u001b[0m-99.00659348708476 \n",
      "INFO:tensorflow:./checkpoint/28_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/28_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m29 \u001b[94mIPS: \u001b[0m-0.8640146097339596 \u001b[94mDM: \u001b[0m-7.117729663848877 \u001b[94mDR: \u001b[0m-3.4149643205129747 \u001b[94mWIS: \u001b[0m-98.96182683832285 \u001b[94mSequential-DR: \u001b[0m-99.00644165720857 \n",
      "INFO:tensorflow:./checkpoint/29_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/29_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m30 \u001b[94mIPS: \u001b[0m-0.8634548222166484 \u001b[94mDM: \u001b[0m-7.146142482757568 \u001b[94mDR: \u001b[0m-3.4089137114616657 \u001b[94mWIS: \u001b[0m-98.43357105544297 \u001b[94mSequential-DR: \u001b[0m-99.02341544090862 \n",
      "INFO:tensorflow:./checkpoint/30_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/30_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m31 \u001b[94mIPS: \u001b[0m-0.8671938050423855 \u001b[94mDM: \u001b[0m-7.131582260131836 \u001b[94mDR: \u001b[0m-3.3993128710752503 \u001b[94mWIS: \u001b[0m-83.3470344386543 \u001b[94mSequential-DR: \u001b[0m-98.97239999647694 \n",
      "INFO:tensorflow:./checkpoint/31_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/31_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m32 \u001b[94mIPS: \u001b[0m-0.8584516902790061 \u001b[94mDM: \u001b[0m-7.002957344055176 \u001b[94mDR: \u001b[0m-3.3881791982331126 \u001b[94mWIS: \u001b[0m-99.3354825480995 \u001b[94mSequential-DR: \u001b[0m-98.77631928514782 \n",
      "INFO:tensorflow:./checkpoint/32_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/32_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m33 \u001b[94mIPS: \u001b[0m-0.8635449200897655 \u001b[94mDM: \u001b[0m-7.114758014678955 \u001b[94mDR: \u001b[0m-3.4296058551686635 \u001b[94mWIS: \u001b[0m-98.90262838543394 \u001b[94mSequential-DR: \u001b[0m-98.94610606486238 \n",
      "INFO:tensorflow:./checkpoint/33_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/33_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m34 \u001b[94mIPS: \u001b[0m-0.8689539109625974 \u001b[94mDM: \u001b[0m-7.171296119689941 \u001b[94mDR: \u001b[0m-3.471189229778747 \u001b[94mWIS: \u001b[0m-70.68976961491867 \u001b[94mSequential-DR: \u001b[0m-98.92406395664024 \n",
      "INFO:tensorflow:./checkpoint/34_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/34_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m35 \u001b[94mIPS: \u001b[0m-0.8662159961660267 \u001b[94mDM: \u001b[0m-7.111026763916016 \u001b[94mDR: \u001b[0m-3.420302761995904 \u001b[94mWIS: \u001b[0m-70.79465550714923 \u001b[94mSequential-DR: \u001b[0m-98.88323666461667 \n",
      "INFO:tensorflow:./checkpoint/35_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/35_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m36 \u001b[94mIPS: \u001b[0m-0.8649206171500269 \u001b[94mDM: \u001b[0m-7.120275497436523 \u001b[94mDR: \u001b[0m-3.4367711757682655 \u001b[94mWIS: \u001b[0m-81.76731257047526 \u001b[94mSequential-DR: \u001b[0m-98.88898935451417 \n",
      "INFO:tensorflow:./checkpoint/36_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/36_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m37 \u001b[94mIPS: \u001b[0m-0.8720269649662306 \u001b[94mDM: \u001b[0m-7.171637535095215 \u001b[94mDR: \u001b[0m-3.4377468824593738 \u001b[94mWIS: \u001b[0m-60.7987883270731 \u001b[94mSequential-DR: \u001b[0m-98.9387122494057 \n",
      "INFO:tensorflow:./checkpoint/37_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/37_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m38 \u001b[94mIPS: \u001b[0m-0.8668852418943405 \u001b[94mDM: \u001b[0m-7.0747809410095215 \u001b[94mDR: \u001b[0m-3.2887216887769686 \u001b[94mWIS: \u001b[0m-67.55792780888876 \u001b[94mSequential-DR: \u001b[0m-98.89597651833378 \n",
      "INFO:tensorflow:./checkpoint/38_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/38_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m39 \u001b[94mIPS: \u001b[0m-0.8687710142899071 \u001b[94mDM: \u001b[0m-7.191899299621582 \u001b[94mDR: \u001b[0m-3.435675186235899 \u001b[94mWIS: \u001b[0m-61.70642692816983 \u001b[94mSequential-DR: \u001b[0m-98.8582363546674 \n",
      "INFO:tensorflow:./checkpoint/39_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/39_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m40 \u001b[94mIPS: \u001b[0m-0.8689889933788839 \u001b[94mDM: \u001b[0m-7.126804351806641 \u001b[94mDR: \u001b[0m-3.3159721557322763 \u001b[94mWIS: \u001b[0m-61.262719185514705 \u001b[94mSequential-DR: \u001b[0m-98.77329156730373 \n",
      "INFO:tensorflow:./checkpoint/40_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/40_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m41 \u001b[94mIPS: \u001b[0m-0.8686463154536826 \u001b[94mDM: \u001b[0m-7.135366439819336 \u001b[94mDR: \u001b[0m-3.3918460417165495 \u001b[94mWIS: \u001b[0m-60.73013851073239 \u001b[94mSequential-DR: \u001b[0m-98.82329792170127 \n",
      "INFO:tensorflow:./checkpoint/41_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/41_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m42 \u001b[94mIPS: \u001b[0m-0.8712137922715545 \u001b[94mDM: \u001b[0m-7.179204940795898 \u001b[94mDR: \u001b[0m-3.3850857733026194 \u001b[94mWIS: \u001b[0m-60.68307052503349 \u001b[94mSequential-DR: \u001b[0m-98.8730693829311 \n",
      "INFO:tensorflow:./checkpoint/42_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/42_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m43 \u001b[94mIPS: \u001b[0m-0.8737548447369863 \u001b[94mDM: \u001b[0m-7.139647483825684 \u001b[94mDR: \u001b[0m-3.410068187142582 \u001b[94mWIS: \u001b[0m-60.71556068873465 \u001b[94mSequential-DR: \u001b[0m-98.84824808293521 \n",
      "INFO:tensorflow:./checkpoint/43_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/43_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m44 \u001b[94mIPS: \u001b[0m-0.8673233942668827 \u001b[94mDM: \u001b[0m-7.089919090270996 \u001b[94mDR: \u001b[0m-3.354643104780843 \u001b[94mWIS: \u001b[0m-60.94543127140705 \u001b[94mSequential-DR: \u001b[0m-98.68315712435216 \n",
      "INFO:tensorflow:./checkpoint/44_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/44_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m45 \u001b[94mIPS: \u001b[0m-0.8708764842362955 \u001b[94mDM: \u001b[0m-7.145613193511963 \u001b[94mDR: \u001b[0m-3.355547013715549 \u001b[94mWIS: \u001b[0m-60.687312432479466 \u001b[94mSequential-DR: \u001b[0m-98.78623032860784 \n",
      "INFO:tensorflow:./checkpoint/45_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/45_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m46 \u001b[94mIPS: \u001b[0m-0.8722612762077572 \u001b[94mDM: \u001b[0m-7.1354498863220215 \u001b[94mDR: \u001b[0m-3.3581265606801303 \u001b[94mWIS: \u001b[0m-60.64099482763919 \u001b[94mSequential-DR: \u001b[0m-98.65088017876975 \n",
      "INFO:tensorflow:./checkpoint/46_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/46_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m47 \u001b[94mIPS: \u001b[0m-0.8731648430330494 \u001b[94mDM: \u001b[0m-7.116348743438721 \u001b[94mDR: \u001b[0m-3.397533417352161 \u001b[94mWIS: \u001b[0m-60.501723870233775 \u001b[94mSequential-DR: \u001b[0m-98.6706238525174 \n",
      "INFO:tensorflow:./checkpoint/47_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/47_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m48 \u001b[94mIPS: \u001b[0m-0.8734855358789192 \u001b[94mDM: \u001b[0m-7.190320014953613 \u001b[94mDR: \u001b[0m-3.4364586425104475 \u001b[94mWIS: \u001b[0m-60.460311215329355 \u001b[94mSequential-DR: \u001b[0m-98.58182372377753 \n",
      "INFO:tensorflow:./checkpoint/48_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/48_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m49 \u001b[94mIPS: \u001b[0m-0.8750630713901149 \u001b[94mDM: \u001b[0m-7.138434886932373 \u001b[94mDR: \u001b[0m-3.3706386559228565 \u001b[94mWIS: \u001b[0m-60.534208732172885 \u001b[94mSequential-DR: \u001b[0m-98.51251875736774 \n",
      "INFO:tensorflow:./checkpoint/49_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/49_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m50 \u001b[94mIPS: \u001b[0m-0.8749004404280426 \u001b[94mDM: \u001b[0m-7.136446952819824 \u001b[94mDR: \u001b[0m-3.3983511118309866 \u001b[94mWIS: \u001b[0m-60.479204681043726 \u001b[94mSequential-DR: \u001b[0m-98.54766851535472 \n",
      "INFO:tensorflow:./checkpoint/50_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/50_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m51 \u001b[94mIPS: \u001b[0m-0.8738711068393805 \u001b[94mDM: \u001b[0m-7.176098346710205 \u001b[94mDR: \u001b[0m-3.373919997917522 \u001b[94mWIS: \u001b[0m-60.11213000457808 \u001b[94mSequential-DR: \u001b[0m-98.56763572571744 \n",
      "INFO:tensorflow:./checkpoint/51_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/51_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m52 \u001b[94mIPS: \u001b[0m-0.8794918409826066 \u001b[94mDM: \u001b[0m-7.214757442474365 \u001b[94mDR: \u001b[0m-3.565554285617895 \u001b[94mWIS: \u001b[0m-60.516403482772134 \u001b[94mSequential-DR: \u001b[0m-98.52058024673526 \n",
      "INFO:tensorflow:./checkpoint/52_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/52_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m53 \u001b[94mIPS: \u001b[0m-0.8780532824978656 \u001b[94mDM: \u001b[0m-7.2137041091918945 \u001b[94mDR: \u001b[0m-3.482081599568536 \u001b[94mWIS: \u001b[0m-60.4205887667032 \u001b[94mSequential-DR: \u001b[0m-98.5194581801334 \n",
      "INFO:tensorflow:./checkpoint/53_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/53_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m54 \u001b[94mIPS: \u001b[0m-0.8746697325133086 \u001b[94mDM: \u001b[0m-7.161681175231934 \u001b[94mDR: \u001b[0m-3.4114603867542543 \u001b[94mWIS: \u001b[0m-59.93982510521378 \u001b[94mSequential-DR: \u001b[0m-98.49612932343157 \n",
      "INFO:tensorflow:./checkpoint/54_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/54_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m55 \u001b[94mIPS: \u001b[0m-0.8871427151454876 \u001b[94mDM: \u001b[0m-7.280055522918701 \u001b[94mDR: \u001b[0m-3.7198387522576373 \u001b[94mWIS: \u001b[0m-60.45529014574257 \u001b[94mSequential-DR: \u001b[0m-98.47112489875087 \n",
      "INFO:tensorflow:./checkpoint/55_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/55_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m56 \u001b[94mIPS: \u001b[0m-0.8788396870460626 \u001b[94mDM: \u001b[0m-7.148619651794434 \u001b[94mDR: \u001b[0m-3.435296439903059 \u001b[94mWIS: \u001b[0m-60.37286641328895 \u001b[94mSequential-DR: \u001b[0m-98.45714092886398 \n",
      "INFO:tensorflow:./checkpoint/56_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/56_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m57 \u001b[94mIPS: \u001b[0m-0.8799823089773836 \u001b[94mDM: \u001b[0m-7.179749488830566 \u001b[94mDR: \u001b[0m-3.3992476032396572 \u001b[94mWIS: \u001b[0m-60.28341266243701 \u001b[94mSequential-DR: \u001b[0m-98.48833965405464 \n",
      "INFO:tensorflow:./checkpoint/57_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/57_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m58 \u001b[94mIPS: \u001b[0m-0.8909614323253412 \u001b[94mDM: \u001b[0m-7.300966739654541 \u001b[94mDR: \u001b[0m-3.6423071286441964 \u001b[94mWIS: \u001b[0m-60.45580228280769 \u001b[94mSequential-DR: \u001b[0m-98.59276594803515 \n",
      "INFO:tensorflow:./checkpoint/58_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/58_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m59 \u001b[94mIPS: \u001b[0m-0.8807699696555651 \u001b[94mDM: \u001b[0m-7.154679775238037 \u001b[94mDR: \u001b[0m-3.3824812550727588 \u001b[94mWIS: \u001b[0m-60.455185847866964 \u001b[94mSequential-DR: \u001b[0m-98.28321085876911 \n",
      "INFO:tensorflow:./checkpoint/59_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/59_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m60 \u001b[94mIPS: \u001b[0m-0.8850991188696361 \u001b[94mDM: \u001b[0m-7.267075061798096 \u001b[94mDR: \u001b[0m-3.5108769661481483 \u001b[94mWIS: \u001b[0m-60.27249683866989 \u001b[94mSequential-DR: \u001b[0m-98.48739340011035 \n",
      "INFO:tensorflow:./checkpoint/60_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/60_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m61 \u001b[94mIPS: \u001b[0m-0.8913531530502357 \u001b[94mDM: \u001b[0m-7.304626941680908 \u001b[94mDR: \u001b[0m-3.6262579714691214 \u001b[94mWIS: \u001b[0m-60.56209876149684 \u001b[94mSequential-DR: \u001b[0m-98.50385019650767 \n",
      "INFO:tensorflow:./checkpoint/61_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/61_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m62 \u001b[94mIPS: \u001b[0m-0.8858667360750937 \u001b[94mDM: \u001b[0m-7.220885276794434 \u001b[94mDR: \u001b[0m-3.4859231100104764 \u001b[94mWIS: \u001b[0m-60.482470907584585 \u001b[94mSequential-DR: \u001b[0m-98.37859110459186 \n",
      "INFO:tensorflow:./checkpoint/62_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/62_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m63 \u001b[94mIPS: \u001b[0m-0.8853522364922549 \u001b[94mDM: \u001b[0m-7.2516021728515625 \u001b[94mDR: \u001b[0m-3.4885909204021797 \u001b[94mWIS: \u001b[0m-60.315076164722974 \u001b[94mSequential-DR: \u001b[0m-98.42877344735263 \n",
      "INFO:tensorflow:./checkpoint/63_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/63_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m64 \u001b[94mIPS: \u001b[0m-0.8991973754105791 \u001b[94mDM: \u001b[0m-7.37738561630249 \u001b[94mDR: \u001b[0m-3.7911961103501413 \u001b[94mWIS: \u001b[0m-60.619039189312936 \u001b[94mSequential-DR: \u001b[0m-98.68209529583572 \n",
      "INFO:tensorflow:./checkpoint/64_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/64_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m65 \u001b[94mIPS: \u001b[0m-0.8904530764527784 \u001b[94mDM: \u001b[0m-7.283432960510254 \u001b[94mDR: \u001b[0m-3.591184623793263 \u001b[94mWIS: \u001b[0m-60.63030585743107 \u001b[94mSequential-DR: \u001b[0m-98.37533621803757 \n",
      "INFO:tensorflow:./checkpoint/65_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/65_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m66 \u001b[94mIPS: \u001b[0m-0.8875470946542918 \u001b[94mDM: \u001b[0m-7.218056678771973 \u001b[94mDR: \u001b[0m-3.517342518158173 \u001b[94mWIS: \u001b[0m-60.44883213547846 \u001b[94mSequential-DR: \u001b[0m-98.41078781987375 \n",
      "INFO:tensorflow:./checkpoint/66_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/66_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m67 \u001b[94mIPS: \u001b[0m-0.8995575854936063 \u001b[94mDM: \u001b[0m-7.357579231262207 \u001b[94mDR: \u001b[0m-3.75771827613918 \u001b[94mWIS: \u001b[0m-60.633159675223425 \u001b[94mSequential-DR: \u001b[0m-98.6979790910487 \n",
      "INFO:tensorflow:./checkpoint/67_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/67_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m68 \u001b[94mIPS: \u001b[0m-0.8929623661170185 \u001b[94mDM: \u001b[0m-7.352140426635742 \u001b[94mDR: \u001b[0m-3.639630033165791 \u001b[94mWIS: \u001b[0m-60.544321416353355 \u001b[94mSequential-DR: \u001b[0m-98.33270499316012 \n",
      "INFO:tensorflow:./checkpoint/68_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/68_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m69 \u001b[94mIPS: \u001b[0m-0.8952716461690226 \u001b[94mDM: \u001b[0m-7.355649948120117 \u001b[94mDR: \u001b[0m-3.669441072684275 \u001b[94mWIS: \u001b[0m-60.602224133707175 \u001b[94mSequential-DR: \u001b[0m-98.48868470955205 \n",
      "INFO:tensorflow:./checkpoint/69_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/69_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m70 \u001b[94mIPS: \u001b[0m-0.8942623094050755 \u001b[94mDM: \u001b[0m-7.320365905761719 \u001b[94mDR: \u001b[0m-3.599522075318363 \u001b[94mWIS: \u001b[0m-60.58810094981052 \u001b[94mSequential-DR: \u001b[0m-98.4889207721015 \n",
      "INFO:tensorflow:./checkpoint/70_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/70_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m71 \u001b[94mIPS: \u001b[0m-0.8838402641692769 \u001b[94mDM: \u001b[0m-7.140829563140869 \u001b[94mDR: \u001b[0m-3.3866252663397023 \u001b[94mWIS: \u001b[0m-60.4632834021664 \u001b[94mSequential-DR: \u001b[0m-97.8536645509196 \n",
      "INFO:tensorflow:./checkpoint/71_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/71_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m72 \u001b[94mIPS: \u001b[0m-0.9001852673475086 \u001b[94mDM: \u001b[0m-7.4071149826049805 \u001b[94mDR: \u001b[0m-3.8623383755599496 \u001b[94mWIS: \u001b[0m-60.6156236711119 \u001b[94mSequential-DR: \u001b[0m-98.35281843521093 \n",
      "INFO:tensorflow:./checkpoint/72_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/72_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m73 \u001b[94mIPS: \u001b[0m-0.8941575009409074 \u001b[94mDM: \u001b[0m-7.2786712646484375 \u001b[94mDR: \u001b[0m-3.624156239603007 \u001b[94mWIS: \u001b[0m-60.57517165127425 \u001b[94mSequential-DR: \u001b[0m-98.26024898438226 \n",
      "INFO:tensorflow:./checkpoint/73_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/73_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m74 \u001b[94mIPS: \u001b[0m-0.8951906869910556 \u001b[94mDM: \u001b[0m-7.327068328857422 \u001b[94mDR: \u001b[0m-3.6496746168939205 \u001b[94mWIS: \u001b[0m-60.58618687243636 \u001b[94mSequential-DR: \u001b[0m-98.00244764281342 \n",
      "INFO:tensorflow:./checkpoint/74_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/74_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m75 \u001b[94mIPS: \u001b[0m-0.9000552194956029 \u001b[94mDM: \u001b[0m-7.364983081817627 \u001b[94mDR: \u001b[0m-3.7942245437502526 \u001b[94mWIS: \u001b[0m-60.62454434174176 \u001b[94mSequential-DR: \u001b[0m-98.04676973396579 \n",
      "INFO:tensorflow:./checkpoint/75_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/75_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m76 \u001b[94mIPS: \u001b[0m-0.8950695655023504 \u001b[94mDM: \u001b[0m-7.272815704345703 \u001b[94mDR: \u001b[0m-3.6816395486131 \u001b[94mWIS: \u001b[0m-60.63980498645234 \u001b[94mSequential-DR: \u001b[0m-98.22764205332173 \n",
      "INFO:tensorflow:./checkpoint/76_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/76_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m77 \u001b[94mIPS: \u001b[0m-0.9035480438197395 \u001b[94mDM: \u001b[0m-7.365461826324463 \u001b[94mDR: \u001b[0m-3.881944230740768 \u001b[94mWIS: \u001b[0m-60.70136193788461 \u001b[94mSequential-DR: \u001b[0m-97.85243394918464 \n",
      "INFO:tensorflow:./checkpoint/77_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/77_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m78 \u001b[94mIPS: \u001b[0m-0.8986001651184048 \u001b[94mDM: \u001b[0m-7.287820816040039 \u001b[94mDR: \u001b[0m-3.7935095626017112 \u001b[94mWIS: \u001b[0m-60.64470181934165 \u001b[94mSequential-DR: \u001b[0m-97.932546806538 \n",
      "INFO:tensorflow:./checkpoint/78_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/78_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m79 \u001b[94mIPS: \u001b[0m-0.9038413355690352 \u001b[94mDM: \u001b[0m-7.38912296295166 \u001b[94mDR: \u001b[0m-3.890746688623826 \u001b[94mWIS: \u001b[0m-60.67487576384852 \u001b[94mSequential-DR: \u001b[0m-97.92477247678605 \n",
      "INFO:tensorflow:./checkpoint/79_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/79_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m80 \u001b[94mIPS: \u001b[0m-0.9014914285501968 \u001b[94mDM: \u001b[0m-7.2829155921936035 \u001b[94mDR: \u001b[0m-3.9727023851753986 \u001b[94mWIS: \u001b[0m-60.693479650695146 \u001b[94mSequential-DR: \u001b[0m-97.34737299182272 \n",
      "INFO:tensorflow:./checkpoint/80_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/80_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m81 \u001b[94mIPS: \u001b[0m-0.8999945310386634 \u001b[94mDM: \u001b[0m-7.30437707901001 \u001b[94mDR: \u001b[0m-3.7631597280406126 \u001b[94mWIS: \u001b[0m-60.66707365622654 \u001b[94mSequential-DR: \u001b[0m-97.50744831198782 \n",
      "INFO:tensorflow:./checkpoint/81_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/81_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m82 \u001b[94mIPS: \u001b[0m-0.9011367727890018 \u001b[94mDM: \u001b[0m-7.303478717803955 \u001b[94mDR: \u001b[0m-3.8254807208091473 \u001b[94mWIS: \u001b[0m-60.668701547442204 \u001b[94mSequential-DR: \u001b[0m-97.64979834762944 \n",
      "INFO:tensorflow:./checkpoint/82_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/82_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m83 \u001b[94mIPS: \u001b[0m-0.9010195864621714 \u001b[94mDM: \u001b[0m-7.387458801269531 \u001b[94mDR: \u001b[0m-3.9254617474723044 \u001b[94mWIS: \u001b[0m-60.65268899857475 \u001b[94mSequential-DR: \u001b[0m-96.85686904472908 \n",
      "INFO:tensorflow:./checkpoint/83_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/83_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m84 \u001b[94mIPS: \u001b[0m-0.9059774505237673 \u001b[94mDM: \u001b[0m-7.406417369842529 \u001b[94mDR: \u001b[0m-4.052030365204548 \u001b[94mWIS: \u001b[0m-60.6800523119745 \u001b[94mSequential-DR: \u001b[0m-97.04212095128885 \n",
      "INFO:tensorflow:./checkpoint/84_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/84_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m85 \u001b[94mIPS: \u001b[0m-0.9060381002855777 \u001b[94mDM: \u001b[0m-7.341787338256836 \u001b[94mDR: \u001b[0m-4.117115626190247 \u001b[94mWIS: \u001b[0m-60.68487539858955 \u001b[94mSequential-DR: \u001b[0m-97.07108658747053 \n",
      "INFO:tensorflow:./checkpoint/85_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/85_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m86 \u001b[94mIPS: \u001b[0m-0.9045803965568066 \u001b[94mDM: \u001b[0m-7.259843349456787 \u001b[94mDR: \u001b[0m-4.033409220082204 \u001b[94mWIS: \u001b[0m-60.61843537518291 \u001b[94mSequential-DR: \u001b[0m-96.56557402454669 \n",
      "INFO:tensorflow:./checkpoint/86_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/86_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m87 \u001b[94mIPS: \u001b[0m-0.9042835877908932 \u001b[94mDM: \u001b[0m-7.271509170532227 \u001b[94mDR: \u001b[0m-4.095175628545043 \u001b[94mWIS: \u001b[0m-60.66168914886375 \u001b[94mSequential-DR: \u001b[0m-96.4927378196131 \n",
      "INFO:tensorflow:./checkpoint/87_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/87_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m88 \u001b[94mIPS: \u001b[0m-0.9068981593568072 \u001b[94mDM: \u001b[0m-7.271401405334473 \u001b[94mDR: \u001b[0m-4.21769593224229 \u001b[94mWIS: \u001b[0m-60.64457648903781 \u001b[94mSequential-DR: \u001b[0m-96.70168732232395 \n",
      "INFO:tensorflow:./checkpoint/88_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/88_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m89 \u001b[94mIPS: \u001b[0m-0.9077716361403767 \u001b[94mDM: \u001b[0m-7.245108604431152 \u001b[94mDR: \u001b[0m-4.1551771560921065 \u001b[94mWIS: \u001b[0m-60.673900092094186 \u001b[94mSequential-DR: \u001b[0m-95.97594602235475 \n",
      "INFO:tensorflow:./checkpoint/89_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/89_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m90 \u001b[94mIPS: \u001b[0m-0.9078863273143197 \u001b[94mDM: \u001b[0m-7.265354633331299 \u001b[94mDR: \u001b[0m-4.150710365737767 \u001b[94mWIS: \u001b[0m-60.677929430931314 \u001b[94mSequential-DR: \u001b[0m-96.23395197116281 \n",
      "INFO:tensorflow:./checkpoint/90_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/90_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m91 \u001b[94mIPS: \u001b[0m-0.9130235713740895 \u001b[94mDM: \u001b[0m-7.2826642990112305 \u001b[94mDR: \u001b[0m-4.384845248917056 \u001b[94mWIS: \u001b[0m-60.692108907742806 \u001b[94mSequential-DR: \u001b[0m-95.99892790240112 \n",
      "INFO:tensorflow:./checkpoint/91_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/91_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m92 \u001b[94mIPS: \u001b[0m-0.9134527029093713 \u001b[94mDM: \u001b[0m-7.3142409324646 \u001b[94mDR: \u001b[0m-4.400238929186203 \u001b[94mWIS: \u001b[0m-60.694377172972125 \u001b[94mSequential-DR: \u001b[0m-95.56352067282839 \n",
      "INFO:tensorflow:./checkpoint/92_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/92_Step-0.ckpt'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m93 \u001b[94mIPS: \u001b[0m-0.9096578218760405 \u001b[94mDM: \u001b[0m-7.200601577758789 \u001b[94mDR: \u001b[0m-4.236998416672068 \u001b[94mWIS: \u001b[0m-60.6800998629592 \u001b[94mSequential-DR: \u001b[0m-95.33862089430488 \n",
      "INFO:tensorflow:./checkpoint/93_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/93_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m94 \u001b[94mIPS: \u001b[0m-0.9196709075397006 \u001b[94mDM: \u001b[0m-7.403517723083496 \u001b[94mDR: \u001b[0m-4.565778293176623 \u001b[94mWIS: \u001b[0m-60.662864183974946 \u001b[94mSequential-DR: \u001b[0m-95.99971155364031 \n",
      "INFO:tensorflow:./checkpoint/94_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/94_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m95 \u001b[94mIPS: \u001b[0m-0.9118234316095112 \u001b[94mDM: \u001b[0m-7.315603733062744 \u001b[94mDR: \u001b[0m-4.312389435007319 \u001b[94mWIS: \u001b[0m-60.68975604842943 \u001b[94mSequential-DR: \u001b[0m-96.5881746843874 \n",
      "INFO:tensorflow:./checkpoint/95_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/95_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m96 \u001b[94mIPS: \u001b[0m-0.9130832387022874 \u001b[94mDM: \u001b[0m-7.317514419555664 \u001b[94mDR: \u001b[0m-4.480210612073225 \u001b[94mWIS: \u001b[0m-60.688822372312245 \u001b[94mSequential-DR: \u001b[0m-96.34750871328295 \n",
      "INFO:tensorflow:./checkpoint/96_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/96_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m97 \u001b[94mIPS: \u001b[0m-0.9181250806398212 \u001b[94mDM: \u001b[0m-7.375402927398682 \u001b[94mDR: \u001b[0m-4.518635079803048 \u001b[94mWIS: \u001b[0m-60.68332641577038 \u001b[94mSequential-DR: \u001b[0m-96.24711252007089 \n",
      "INFO:tensorflow:./checkpoint/97_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/97_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m98 \u001b[94mIPS: \u001b[0m-0.910935320355021 \u001b[94mDM: \u001b[0m-7.298365592956543 \u001b[94mDR: \u001b[0m-4.34956798923443 \u001b[94mWIS: \u001b[0m-60.70222137090041 \u001b[94mSequential-DR: \u001b[0m-96.36021884797098 \n",
      "INFO:tensorflow:./checkpoint/98_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/98_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m99 \u001b[94mIPS: \u001b[0m-0.9191202621987824 \u001b[94mDM: \u001b[0m-7.331487655639648 \u001b[94mDR: \u001b[0m-4.543331956460551 \u001b[94mWIS: \u001b[0m-60.68607118831409 \u001b[94mSequential-DR: \u001b[0m-95.90833113476643 \n",
      "INFO:tensorflow:./checkpoint/99_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./checkpoint/99_Step-0.ckpt'] \n",
      "\u001b[95mOff-Policy Evaluation\u001b[0m - \u001b[94mEpoch: \u001b[0m100 \u001b[94mIPS: \u001b[0m-0.920233068977458 \u001b[94mDM: \u001b[0m-7.4031243324279785 \u001b[94mDR: \u001b[0m-4.61931601723693 \u001b[94mWIS: \u001b[0m-60.66444556522768 \u001b[94mSequential-DR: \u001b[0m-96.56094574458396 \n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # just to clean things up; only needed for the tutorial\n",
    "\n",
    "#########\n",
    "# Agent #\n",
    "#########\n",
    "agent_params = DDQNBCQAgentParameters()\n",
    "agent_params.network_wrappers['main'].batch_size = 128\n",
    "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(100)\n",
    "agent_params.algorithm.discount = 0.99\n",
    "\n",
    "# to jump start the agent's q values, and speed things up, we'll initialize the last Dense layer\n",
    "# with something in the order of the discounted reward of a random policy\n",
    "agent_params.network_wrappers['main'].heads_parameters = \\\n",
    "[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n",
    "\n",
    "# NN configuration\n",
    "agent_params.network_wrappers['main'].learning_rate = 0.0001\n",
    "agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n",
    "\n",
    "# ER - we'll be needing an episodic replay buffer for off-policy evaluation\n",
    "agent_params.memory = EpisodicExperienceReplayParameters()\n",
    "\n",
    "# E-Greedy schedule - there is no exploration in Batch RL. Disabling E-Greedy. \n",
    "agent_params.exploration.epsilon_schedule = LinearSchedule(initial_value=0, final_value=0, decay_steps=1)\n",
    "agent_params.exploration.evaluation_epsilon = 0\n",
    "\n",
    "# can use either a kNN or a NN based model for predicting which actions not to max over in the bellman equation\n",
    "agent_params.algorithm.action_drop_method_parameters = KNNParameters()\n",
    "\n",
    "\n",
    "DATATSET_PATH = 'acrobot_dataset.csv'\n",
    "agent_params.memory = EpisodicExperienceReplayParameters()\n",
    "agent_params.memory.load_memory_from_file_path = CsvDataset(DATATSET_PATH, is_episodic = True)\n",
    "\n",
    "spaces = SpacesDefinition(state=StateSpace({'observation': VectorObservationSpace(shape=6)}),\n",
    "                          goal=None,\n",
    "                          action=DiscreteActionSpace(3),\n",
    "                          reward=RewardSpace(1))\n",
    "\n",
    "graph_manager = BatchRLGraphManager(agent_params=agent_params,\n",
    "                                    env_params=None,\n",
    "                                    spaces_definition=spaces,\n",
    "                                    schedule_params=schedule_params,\n",
    "                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n",
    "                                    reward_model_num_epochs=30,\n",
    "                                    train_to_eval_ratio=0.4)\n",
    "graph_manager.create_graph(task_parameters)\n",
    "graph_manager.improve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection with OPE\n",
    "Running the above preset will train an agent based on the experience in the csv dataset. Note that now we are finally simulating the real scenario with Batch Reinforcement Learning, where we train and evaluate solely based on the recorded dataset. Coach uses the same dataset (after splitting it, obviously) for both training and evaluation. \n",
    "\n",
    "Now that we have ran this preset, we have 100 agents to choose from (one is saved after every training epoch), and we would have to decide which one we choose for deployment (either for running another round of experience collection and training, or for final deployment, meaning going into production).  \n",
    "\n",
    "Openning the experiment csv in Dashboard and displaying the OPE signals, we can now choose a checkpoint file for deployment on the end-node. Here is an example run, where we show the `Weighted Importance Sampling` and `Sequential Doubly Robust` OPEs. \n",
    "</br>\n",
    "![Model Selection](BatchRL/img/model_selection.png \"Model Selection using OPE\") \n",
    "\n",
    "Based on this plot we would probably have chosen a checkpoint from around Epoch 85 (but this is often more of an art than science). From here, if we are not satisfied with the agent's performance we can iteratively continue with data collection, training a new agent (maybe based on a combination of all the data collected so far), and deployment. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
